# prototype/cleaning/clean.py
import duckdb
import pandas as pd
from pathlib import Path

def clean_db(db_path: str, output_dir: str = "data/clean"):
    """
    For each table in the DuckDB at db_path:
      - parse datetime,
      - drop rows with any required-NaNs,
      - drop rows immediately after a gap >1h,
      - write cleaned CSV to output_dir/<table>.csv
    """
    con = duckdb.connect(db_path)
    # find all user tables
    tables = [r[0] for r in con.execute("SHOW TABLES").fetchall()]
    outdir = Path(output_dir)
    outdir.mkdir(parents=True, exist_ok=True)

    for table in tables:
        df = con.execute(f"SELECT * FROM {table}").df()
        # ensure datetime
        df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce")
        df = df.dropna(subset=["datetime"])
        df = df.sort_values("datetime")

        # drop any rows with missing pollutant/weather fields
        # we assume: everything except datetime is required
        req_cols = [c for c in df.columns if c != "datetime"]
        df = df.dropna(subset=req_cols, how="any")

        # drop rows that follow a gap >1 hour
        gaps = df["datetime"].diff() > pd.Timedelta(hours=1)
        if gaps.any():
            df = df.loc[~gaps]

        out_csv = outdir / f"{table}.csv"
        df.to_csv(out_csv, index=False)
        print(f"  • Cleaned `{table}` → {out_csv}")

    con.close()
    print(f"✅ All tables cleaned into {output_dir}")
